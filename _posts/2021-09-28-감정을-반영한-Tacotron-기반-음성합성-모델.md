---
title: 감정을 반영한 Tacotron 기반 음성합성 모델 논문 간단 리뷰
author: HW
date: 2021-09-28 11:47:00 +0800
categories: [Speech Synthesis]
tags: [Emotional TTS]
math: true
image: /assets/img/emotionalTTS/main_emotion.jpg
---

# **Abstract**

- 본 논문은 2017년 NIPS 학회에 발표된 Emotional Speech Synthesiszer 논문이다.

- 당시 **Tacotron의 단점(exposure bias, attention alignment irregular 등)을 보완**하면서, 감정 레이블을 반영하여 감정을 대입한 높은 수준의 완성도를 보이는 음성합성기를 발표했다.

<br/><br/>

# **Introduction**

- 제시하는 이 모델은 **감정이 추가**되고, Tacotron의 end-to-end을 기반으로 한 음성합성 모델이다.

- 기존 Tacotron에는 **exposure bias(노출 편향)** 문제와 **attention alignment가 불규칙한 문제**가 존재한다. 본 논문에서는 문제를 Decoder RNN에서 context vector와 residual connection을 활용하여 해결했다.

  ###### (exposure bias: 특정 단계의 잘못된 결과가 전범위적으로 영향을 미치는 경우)

 <br/><br/>

# **Emotion Synthesizer 개요**

- 기존 Tacotron에 **학습된 emotion embedding을 추가 적용**함으로써 구현했다.

- **AttentionRNN과 DecoderRNN에** 각각 emotion embedding을 적용하는 것이다.

  

- 아래 내용 이외의 경우, 기존 Tacotron과 같은 방향성을 가졌기에 링크를 참고하면 된다.<br><https://hwrg.github.io/posts/딥러닝-기반의-음성합성-모델-Tacotron/>

<br/><br/>

# **Attention 기반 Decoder**

![decoder](/assets/img/emotionalTTS/decoder.png)

- 기존 Tacotron의 RNN 스택은 content기반 **tanh attention**과 함께 사용된다.<br>
   Decoder는 동일하게 Pre-net → Attention RNN → Residual connection RNN 구조를 가진다.

- 모든 미니 배치에 대한 **input은 0인 GO frame으로 제공**된다.

  

- **Pre-net**의 과정은 다음과 같다.
  
  1) output과 attention input 크기가 일치**하도록 **projection 진행 
  2) 1을 위해 레이어를 하나 추가해 **one-hot emotion label vector의 projection을 추가**
  3) 동일한 injection이 decoder RNN의 첫 번째 레이어에서 수행,<br>현재 시점에 생성된 spectrogram에 감정 추가<br><br>

  
  
- 기존의 타코트론은 decoder가 train에선 **직전의 정보를 바탕으로 한 번에 하나의 출력을 예측**할 수 있는데, **test에는 이전 정보를 활용할 수 없다는 단점이 있다.**<br>이렇게 될 경우 오류가 빠르게 누적되고 긴 음성을 생성할 경우 엉망으로 생성된다.

  ![enc_dec_timestep](/assets/img/emotionalTTS/enc_dec_timestep.png)

- **Exposure biased 문제**는 encoder 상태와 decoder 시간 단계에 지저분한 패턴을 만든다.<br>
   → 분리성, attention 정렬 X<br>
   (Encoder timestep이 증가할수록 Decoder timestep도 증가하는 것이 정상 <br> But 불일치하여 그림과 같이 **대각선으로 가다가 흐릿한 직선으로 변화**)

- 이를 해결하기 위해 단조로움을 추가하여 더 깨끗한 패턴을 생성해내어 정렬할 수 있는 **Raffel의 알고리즘***을 적용했다. <br>

  

- **STFT(Semi-Teacher-Forced-Training)**<br>
   Seq2Seq 구조에서 t 시간에서 decoder의 input은 이전 단계 yt-1의 spectrogram 프레임이다.
   만약 **이전 프레임에 노이즈가 발생**하면 이를 그대로 사용할 경우, **exposure biased 문제**가 발생한다.
   그래서 본 논문은 **y1과 yt-1의 평균**을 decoder에 input으로 제공하여 문제를 덜어내고자 했다.<br><br>
   
   

*Online and linear-time attention by enforcing monotonic alignments

 <br/><br/>

## **Experiments**

- **텍스트, 오디오, 감정 level 묶음**을 포함한 한국 데이터셋에 Emotion Tacotron 학습을 진행한다.

- 중립, 분노, 두려움, 행복, 슬픔, 놀라움 **6가지 감정**으로 뉴스 읽기를 하여 21시간 데이터셋을 확보했다.
   행복의 경우 다른 스크립트가 영향을 미치지 않아 포함하지 않았으며, 
   Web RTC 음성 활동 검출기를 활용하여 음성이 빈 부분을 제거했다.

- 끝의 음성을 중요히 훈련하기 위해 문장 끝에 padding된 0에 masking을 하지 않고 진행한다.

- 아래 링크를 통해 데모를 확인할 수 있다. <br>

  <https://youtu.be/bh2HP0n2ik8>
   <br/><br/>

## **Conclusion**

- 문자 sequence와 다양한 emotion을 입력으로 wave 신호를 생성하는  end-to-end 합성 Tacotron을 제안했다.

- 실험을 통해 **음성 품질**이 alignment의 정렬과 clearness가 밀접한 관계가 있음을 확인할 수 있다.<br>
  https://github.com/AzamRabiee/Emotional-TTS
  

<br/><br/>



 
