---
title: 2021 딥러닝 기반 음성 합성 총정리 - 2
author: HW
date: 2021-10-19 17:00:00 +0800
categories: [Speech Synthesis]
tags: [Speech], [Survey]
math: true
image: /assets/img/speech_survey/speech_survey_main.jpg
---

본 게시글은 towards data science 사이트에 Patrick Meyer가 작성한 <br>
‘State Of The Art of Speech Synthesis at the End of May 2021’를 번역 및 정리한 글이다.<br>

<https://hwrg.github.io/posts/2021-딥러닝-기반-음성-합성-총정리-1/><br>
이전 게시글에 이어서 모델 시스템 구조와 데이터셋, 학회, 측정 방법 등을 포함하고 있다.<br>

마찬가지로 글의 출처는 다음과 같다. <br>
<https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2><br><br>

## **7.**  **End-to-End Systems**

**1)**  **Char2Vec**<br>

- 2017년 2월 몬트리올 대학에서 Char2Vec을 발표했다.<br>
   Char2Vec은 Bidirectional RNN, Attention RNN, SampleRNN(Vocoder)을 조합한 모델이다.<br>

- 이 End-to-End 모델은 직접 waveform sample을 학습할 수 있으며,<br>
   중간에 mel-spectrogram을 거친 후 만들어질 필요가 없다.<br>
   오직 input으로 사용된 데이터는 직접적으로 모델의 output이 된다.<br>
   ![char2vec](/assets/img/speech_survey/char2vec.png)

- Char2Vec은 Player와 Neural vocoder로 구성되어 있다. <br>
   이 때, reader는 attention을 포함한 encoder-decoder이다. <br><br>

**2)**  **Deep Voice, FastSpeech 2s**<br>

- 2017년 2월 바이두는 Deep Voice를 발표했다. Deep Voice는 작은 오디오 클립들과 녹음된 데이터셋(transcription)을 통해 학습을 진행한다.<br>
- 2020년 MS는 Mel-spectrogram을 생성하지 않고 스펙트럼을 생성하는 FastSpeech 2s를 발표했다. FastSpeech 2s는 Deep voice보다 조금 낮은 성능을 보인다.<br><br>

**3)**  **End-to-End Review**

- End-to-End가 한 번에 이뤄지는 장점이 있지만, 음성 생성 측면에서는 유용하지 않다.<br>
   그래서 대부분 grapheme-to-phoneme 모델, 지속시간 예측 모델, mel-spectrogram 생성 모델 등으로 특징을 가지는 모델을 각각 설계하고, 마지막에 Vocoder를 따로 둔다. <br>

- 음성 합성 대회 중 하나인 블리자드 챌린지 2020에서 최종 라운드에서 2개의 생성 공간 기반의 SPSS(Statistical Parametric Speech Synthesis)가 발표되며 이 기술이 기존 고전적인 시스템을 누르고 대세가 될 것이라는 전망을 보여주었다.  <br>

- 그외 블리자드 2020에서 다른 팀들 중 절반은 WaveRNN 또는 Wavenet Vocoder를 사용해 Seq-to-Seq 모델을 활용했으며, 절반은 DNN 등 비슷한 Vocoder를 기반의 모델을 활용했다고 한다.<br><br>

 

## **8.**  **Research Actors**

- 음성 합성 관련 최근 71편 논문 중 가장 많은 비중을 차지한 기관은 Web 기반의 회사(Google, Facebook, Naver)이며, 그 다음으로 Tech 중심의 회사(Nvidia, NC Soft)가 비슷한 점유율을 지니고 있다. 마지막으로 대학의 경우 비중이 꽤 낮게 세 번째 순위이다.<br>

- 국가의 경우, 1위는 미국(45%), 2위는 중국(25%), 3위는 한국(14%) 순서로 논문을 출간했다.<br>
   놀랍게도 북미와 아시아를 비교하면 근소하게 3%로 아시아가 앞선다.<br>

- 구글의 자회사 DeepMind(EU)는 Wavenet, Tacotron, WaveRNN, GAN-TTS, EATS 등 13개의 논문을 게재하여 1위이며, 다음으로 DeepVoice와 ClariNet을 제안한 Baidu와 TransformerTTS와 FastSpeech를 제안한 MS가 공동 2위를 차지했다.<br><br>

 

## **9.**  **Find Datasets**

**1)   LJ Speech**<br>

- 가장 많은 모델 평가에서 활용되는 음성 데이터셋이다. 한 명의 화자가 짧은 지문을 읽어 13100개의 음성 데이터로 이루어져 있다.<br>

**2)   LibriTTS**<br>

- LibriTTS는 Heiga Zen이 google speech와 google brain의 도움을 받아 24kHz의 샘플링 속도로 585시간의 영어 음성 데이터셋이다.<br>

**3)   Common Voice 6.1**<br>

- 다국어로, 모질라에서 모든 사람들이 음성 인식에 대한 접근 권한을 허용하는 데이터베이스를 구축하기 위해 제작을 시작했다. 프랑스어 버전으로 682시간이 존재한다.<br>

**4)   Blizzard Speech Data Base**<br>

- 매년 열리는 블리자드 speech 챌린지 대회에 훈련할 수 있는 여러 음성 샘플을 제공한다.<br>
   대회에 참여하는 사람에게만 데 이터셋이 제공된다.<br>

**5)   CMU-Arctic**<br>

- Project Gutenberg 텍스트에 신중히 선택된 문장 1150개 로 구성되며, 저작권이 자유롭다.<br>

이외에도 OpenSLR, 오디오북 등 다양한 데이터셋이 존재하지만, 모델에서 운율 또는 감정적인 측면에 대해 훈련시킬 수 있는 데이터셋을 찾는 것은 굉장히 어렵다.<br><br>

 

## **10.**  **Measure Quality**

- 음성 생성의 경우, 모델의 성능을 측정하기 위해 정형화된 테스트가 존재하지 않다. <br>
   음성의 품질은 naturalness, robustness(단어를 잊어버리거나 중복 생성하는지), accuracy를 포함한 다양한 방면에서 평가가 진행된다. <br>

- 품질 테스트는 인간이 진행하며, 평가자에게 음성 frequency의 오디오 품질을 평가를 부탁하고, MOS(평균 의견 점수)를 통해 1점 ~ 5점으로 주관적인 판단을 맡긴다.<br>
  ![measure_quality1](/assets/img/speech_survey/measure_quality1.png)

- 또다른 방법으로 2011년부터 크라우드소싱 방식에 기반하여 잘 설명된 작업 방식으로 이점을 살릴 수 있었다. 가장 잘 알려진 것은 크라우드모스로 불리는 프레임워크이며, Amazon Mechanical Tunk 사이트를 활용하면 된다.<br>

- 대부분의 연구실에서는 이 원리를 활용해 알고리즘 평가를 진행하며, 각 모델에 대한 성능을 전반적으로 확인할 수 있다. 평가 결과는 스피커와 녹음 상태에 따라 크게 좌우된다.<br>

  ![measure_quality2](/assets/img/speech_survey/measure_quality2.png)<br><br>



## **11.**  **Voice Conference**

**1)**  **InterSpeech**<br>

- InterSpeech는 1988년에 설립되었으며, 8월 말, 9월 초에 개최된다. InterSpeech를 개최하는 기관 ISCA(국제음성통신협회)의 목표는 음성통신의 과학기술과 관련된 모든 분야의 교류를 촉진하는 것이다.<br>

**2)**  **ICASSP**<br>

- ICASSP는 매년 6월에 개최된다. IEEE가 주관하는 학회이며, 음향, 음성 및 신호처리 관련 주제로 진행된다.<br>

- 그 외에 ICLR, ICML, Nerulips 등 머신러닝과 딥러닝 중심의 컨퍼런스에서도 음성 관련 논문이 게재된다.<br><br>

  

## **12.**  **Next Challenges**

- 딥러닝 기반으로 상당한 성능의 발전을 이뤄냈지만, 아직 많은 과제가 남아있다.<br>
   대화 과정에서 상황에 맞는 억양과 긴 휴식기간 없는 자연스러움이 중요하다. <br>
   이를 충족하기 위해 음성 생성은 즉각적이어야 한다.<br>
- 생성 과정에서 사용자에게 신호를 받고, 다시 제공하기 위해 많은 경우에서 생성이 끝날 때까지 기다려야 하는 경우가 많다. (변환할 전체 문장을 수신하기 때문)<br>
- 이런 점에서 최근 발표된 Non-AutoRegressive 모델은 여러 활동을 병렬 처리를 진행하여 몇 번의 음성 신호를 생산할 수 있어 기존의 모델을 능가한다고 볼 수 있다. 이들은 문장 길이에 상관없이 밀리초 단위로 생성된다. (Real-Time Factor)<br><br>

**1)**  **Model Lightweight**<br>

- 모바일 디바이스에서 음성 합성 기능이 적용될 경우 경량화가 핵심이다.<br>
   DeviceTTS(알리바바 20년 10월)는 150만개의 파라미터를 활용하여 1350만개의 파라미터를 활용하는 Tacotron과 비슷한 품질의 음성을 생성할 수 있다.<br>

- LightSpeech2(MS 21년 2월)는 FastSpeech2 기반으로 180만개 매개변수를 활용하는데, FastSpeech2의 2700만개의 파라미터 대비 큰 폭으로 절약한 것을 알 수 있다.<br>

**2)**  **Emotional<br>**

- 단조로운 음성 데이터셋에 지속시간과 리듬에 변화를 시스템에 나타내기 위한 음성 합성 Markup 언어가 있는데, 피치와 contour(윤곽), 지속시간, 볼륨 등 특성을 적용할 수 있다.<br>
- 최근 서울대와 SKT에서 공동으로 게재한 논문 ‘Expressive TTS using style tag’에서는 태그를 예상되는 운율 결정 기술을 활용하여 대체하여 적용했다.<br>

**3)**  **Multi-speaker, Multi-style**<br>

- Zero-shot TTS의 방식은 몇 초의 음성에 의존하여 네트워크를 새로운 음성에 각색하는 내용을 포함하고 있다. 이는 음성 복제와 유사하다. 이와 관련해 ‘The Multi-speaker Multi-style Voice Cloning Challenge’라는 대회가 존재하는데, 이 대회는 한 음성을 동일한 언어 또는 타국 언어로 최대한 유사하게 복제해 내는 대회이다.<br>

**4)**  **Voice clone**<br>

- 21년 10월 기준 음성 합성 모델은 복제할 사람의 음성을 초단위로 학습하여 음성을 재현할 수 있다. 목소리를 잃은 사람도 생성을 위해 몇 분의 녹음만 있으면 가능한 일이다.<br>

- 물론 중요한 인물의 목소리를 취하여 사칭의 위험이 있다는 점에서 가장 중요한 다음 과제는 불법 합성을 감지하는 시스템을 제공하는 것이다.<br><br>

 

## **13.**  **Conclusion**

- 우리 일상 속 자연스러운 목소리가 사실 기계가 만들어낸 목소리와 동일하다고 생각할 수준에 도달했다. 그럼에도 아직 더 발전할 여지가 있다는 점에서 굉장히 흥미롭다. 일부 기업과 대학의 연구소들은 음성 합성의 미래를 보고 품질 저하 없이 생성 속도가 사람보다 빠른, 그리고 하나의 화자에서 여러 음성을 추출하는 등 상업적으로 활용하기 위해 끊임없이 연구하고 있다.<br>

- 음성 합성 기술이 구글이 2018년 5월 공개한 발표에서 인공지능 미용실 예약과 같은 시스템이 우리 삶에 자연스레 녹아들고, 또 이 기술을 통해 어떤 흥미로운 아이디어가 나타나 우리 삶에 변화를 줄지 기대된다.<br><br>