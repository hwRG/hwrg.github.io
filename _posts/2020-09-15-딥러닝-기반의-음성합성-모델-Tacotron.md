---
title: 딥러닝 기반의 음성합성 모델 - Tacotron
author: HW
date: 2020-09-15 17:00:00 +0800
categories: [Speech Synthesis]
tags: [Tacotron]
math: true
image: /assets/img/insert/tacotron/tacotron_model.jpg

---



# **들어가면서**

![exTTS](/assets/img/insert/tacotron/taco.jpg)

<center>(출처: https://www.pinterest.com.au/pin/332210910020186329)</center>


어느 순간 갑자기 기계 목소리가 자연스러워졌음을 한 번쯤은 느낀 적 있을 것이다. <br/>

내 경우엔 느긋한 구글 Assistant의 목소리 때문에 그랬는지 잘 체감을 못했는데,

어쨌든 갑자기 부드럽게 음성을 제작할 수 있게 된 것엔 다 이유가 있었다. <br/>

그 대표적인 예로 이 Tacotron이 전 세계의 모든 음성 합성 기술이 대폭 발전하는 데에 많은 기여를 했다.

드디어 신경망을 음성을 합치는데 까지 도달한 것이다.

그래서 구글에서 발표한 이 대단한 모델에 대해 설명하고자 한다.<br/><br/>





# Tacotron의 역할

Tacotron은 제목에서 언급했듯이 음성을 합성하기 위해 고안된 모델이다.<br/>

<br>

![translateHI](/assets/img/insert/tacotron/translateHI.png)

<center> (출처: https://translate.google.com/?sl=ko&tl=en&text=안녕&op=translate)</center>


우리는 위와 같이 번역기를 사용하는데, 대부분의 번역은 이전의 Text에서 이후의 Text로 변환하는 Sequence to Sequence 모델을 기반으로 한다.<br>

음성 합성의 경우에 이 Seq2Seq 모델을 사용하는데,  그 구조는 다음과 같다.

![seq2seq](/assets/img/insert/tacotron/seq2seq.png)

영어 Text를 input으로 넣으면, 이 Text가 vector화 되어 저장되고, 이를 바탕으로 Decoder에서 목표 Text에 대한 처리를 거친 후 프랑스어로 번역되어 output이 나온다.<br><br>

마찬가지로 Tacotron에서도 이 Seq2Seq을 활용하는데, 자세한 내용은 뒤에서 다루고자 한다. <br/><br/><br/>



# Tacotron 구조

![tacotron_model](/assets/img/insert/tacotron/tacotron_model.jpg)

대표 사진으로 걸어둔 사진이 Tacotron의 전체 구조이다.<br>

크게 왼쪽 아래 Character Embeddings 부터 Encoding이 이뤄지고, Attention을 거쳐서 Decoder에서 Mel-Spectrogram을 생성하는 것을 확인할 수 있다. <br>

그리고 마지막으로 CBHG를 한 번 더  거쳐서 Linear-Spectrogram을 만들고 Vocoder에게 이를 넘긴다.

그러면 하나의 완벽한 음성 합성이 완성 된다.

<br/><br/>



# Encoder

인코더는 전처리한 Text Embedding을 Pre-net과 CBHG를 거쳐 Sequences를 얻는 과정을 거친다.

![encoder](/assets/img/insert/tacotron/encoder.png)

순서는 Text Input -> Pre-net -> CBHG -> Output(Vector)이며, 

Pre-Net의 경우 **FC(Dense) – ReLU – Dropout – FC – ReLU – Dropout** 의 단계가 있으며,

CBHG의 경우  **Conv1D bank – Max pooling – Conv1D projections – Highway net – GRU(Bidirectional RNN)** 의 단계로 이루어져 있다. 

각 과정에 대해서는 구글링하여 알 수 있으며, 추후에 블로그에 포스팅하고자 한다.  <br>  

### ***CBHG***

![CBHG_summary](/assets/img/insert/tacotron/CBHG_summary.png)

CBHG를 사용하는 이유는 highway를 사용함으로써 character 단위를 표현하는데 더 효율적으로 되기 때문이다.<br>

최종적으로 highway 4번에 걸쳐 나온 Vector가 Attention에 사용될 query가 된다.

![CBHG_detail](/assets/img/insert/CBHG_detail.png)

<br>

<br>



# Decoder

Decoder에서는 Text에 대한 Vector가 여러 순환 신경망(Attention RNN, Decoder RNN)을 거쳐 Mel spectrogram을 얻는 과정을 한다.<br>

과정은 <GO> frame-> Pre-net -> Attention RNN -> Decoder RNN -> Mel spectrogram 이다.

![decoder](/assets/img/insert/tacotron/decoder.png)

정리하자면, 가장 처음 비어있는 <GO> frame에서 시작하여 Attention의 영향을 받은 Decoder RNN을 거쳐서 Mel-Spectrogram 3개를 생성하여 출력하고, 이를 다음 과정의 Node로 사용하게 된다.<br>

지속적으로 반복하여 얻은 Mel-Sepctrogram을 음성 합성의 재료가 된다.<br><br>



# Attention

Attention은 학습 중인 화자 고유의 발화 특징 또는 문장의 핵심 장소에 집중하여 더 자연스러운 합성이 되도록 하는 역할을 한다.<br>

<br>

![attention](/assets/img/insert/tacotron/attention.png)

예를 들면, '안녕하세요'와 '반갑습니다'

둘 다 '안'과 '반'은 문장의 시작으로 상대적으로 더 뚜렷해야 하고, 

화자마다 강세를 어디에 두는 지에 따라 느낌이 달라지니 이를 위해 필요하다.<br><br>



# Post-Processing net

음성을 합성하기 위한 Linear-Spectrogram을 한 번에 만들어낼 수 있지만 Tacotron에서는 그러지 않았다.

그 이유는 바로 Decoder에서 Linear Spectrogram으로 만들 경우, 상대적으로 Harmonic이 적어서 부자연스럽기 때문이다.

![attention](/assets/img/insert/tacotron/attention.png)

위 사진에서 y축 DFT bin에서 하단 부를 보면  물결 모양이 (b)에서 세세하고 많은 것을 확인할 수 있다.

그렇기 때문에 더 자연스러운 목소리의 Spectrogram을 제작하기 위해 이런 방식을 채택한 것이다.<br><br>



 

# Vocoder(Griffin-Lim)

Tacotron은 만들어진 Spectrogram을 더 자연스럽게 만들어내는 모델(Vocoder)로 Griffin-Lim을 선정했다.

Tacotron 논문이 세상에 나온지 거의 4년이 되어가는 상황에서 수많은 Vocoder들이 탄생하고 있으며, 그 중 Wavenet, Waveglow, MelGAN, VocGAN 등 여러가지 발표되었다. <br>

<br>

워낙 안정적이고 더 성능이 좋은 모델이 출시되었기 때문에 Griffin-Lim은 짧게 넘어가고자 한다.

![griffin-lim](/assets/img/insert/tacotron/griffin-lim.png)

위 과정을 통해 오디오 파일로 변환한다.

