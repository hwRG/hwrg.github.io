---
title: 딥러닝 기반 음성 합성 총정리 - 1
author: HW
date: 2021-10-10 17:00:00 +0800
categories: [Speech Synthesis]
tags: [Speech]
math: true
image: /assets/img/speech_survey/speech_survey_main.jpg
---

<br>본 게시글은 towards data science 사이트에 Patrick Meyer가 작성한 <br>
‘State Of The Art of Speech Synthesis at the End of May 2021’를 번역 및 정리한 글이다.<br>

많은 내용을 담고 있어 2개의 게시글로 나눠 작성할 예정이다. <br>

글의 출처는 다음과 같다. https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2<br><br>

## **0. Introduction**

- 2021년 5월까지 음성합성 연구 기술 현황에 대한 내용 총정리<br>
  Sentence to Signal의 기술의 전반과 모델 소개<br>
- 이전 까지의 자동 음성 합성, 이에 대한 문제<br>
  → 생성을 위한 파이프라인 소개<br>
  → mel-spectrogram에 대한 간단 정리<br>
  → Deep Generative model 소개<br>
  → End to End System 소개<br>
  → 음성 합성을 연구하는 기관 및 사람 등에 대한 설명<br><br>

## **1. TL;DR (요약)**

- 딥러닝 기반의 음성합성의 과정은 크게 두 단계로 진행된다. 첫 번째 단계에서 문장을 주파수로 표현을 생성하여 mel-spectrogram을 만들어 내고, 두 번째 단계에서 mel-spectrogram을 파형으로 만들어, 들을 수 있는 음성을 만든다.<br>
- 최근 음성합성에 널리 활용되고 있는 기술은 입력과 출력 사이 정렬(alignment)을 개선하는 attention과 convolutional network이다. 정렬은 지속시간 예측에 대한 알고리즘에 의해 성능이 향상된다.<br>
- 정렬된 데이터는 vocoder에 의해 3차원 주파수(시간, 주파수, 파워)를 소리로 변경한다. <br>
- 모든 합성 결과물은 학습 데이터에 의존하기 때문에 데이터가 가장 중요하며, 최대한 운율과 리듬, 성격 요소 등 매개 변수화 할 수 있는 음성 데이터를 활용하는 것이 좋다.<br><br>

## **2. One-to-many Problem**

- 음성합성에서의 핵심은 문장을 제시하면 그에 따른 수천 개 값의 파형을 생성하는 것이다.<br>
- 음성에는 음량과 억양, 속도, 처음과 끝, 감정 등 다양한 특성을 보유하고 있다. <br>
  이를 전역(global) 또는 개별(individual)로 훈련하도록 단계를 나누어 다양한 특성들을 처리한다.<br>
  (Tacotron에서 Encoder – Attention – Decoder를 진행한 후 Wavenet을 사용하는 이유)<br>
  ex) Sentence → Phonemes(음운) → Spectrogram → Waveform → Audio Signal<br><br>

## **3. Processing Pipeline**

- Vocoder는 두 번째 단계로, 이 주파수의 표현을 기반으로 생성을 마무리한다.<br>
  생성 네트워크는 Convolution부터 VAE, Attention, GAN 등 순서로 표준이 되었다.<br>

  ![processing_pipeline](/assets/img/speech_survey/processing_pipeline.png)

- 위 그림은 음성을 생성하는데 사용되는 머신러닝 파이프라인 구조다.<br>

- 대부분 딥러닝 합성과 비슷하게 학습 단계와 생성 단계로 구성되는데, 특이한 점은 ‘in between’이 삽입되어 다른 데이터와 음향 모델의 미세 조정을 수행한다.<br>

- 학습 단계에서 파이프라인으로 모델을 생성할 수 있다. 문장은 인코더와 디코더에 입력과 관련된 음성 파일이다. 여기에 화자의 ID가 추가되는 경우가 있다.<br><br>

## **4. In Pipeline**

**1) Training Pipeline and module (학습 단계의 파이프라인)** <br>

- 텍스트 분석 모듈 (Text Analysis module)<br>
  먼저 Input에 대한 텍스트 정규화, 숫자 to 텍스트 변환, 문장 분할, 음절 to 음소 변환, 운율 요소 추가 등을 수행한다.<br>

- 음향 분석 모듈(Acoustic Analysis module)<br>
  Input – 텍스트와 관련된 음향 특성 및 스피커의 ID<br>
  학습 단계에서 기존의 데이터와 생성된 데이터의 특징 간 차이를 분석한다.<br>
  이 때, 음향 feature는 FFT(Fast Fourier Transform)와 같은 신호처리 알고리즘을 사용하여 샘플이 생성된다. <br>이 모듈은 신호 지속시간을 예측하고, 텍스트를 정렬한다.<br>
  최신 논문에서는 이 예측 네트워크를 더 개선하고, 피치에 대한 예측을 더 향상시켰다.<br>

- 학습 단계의 모듈은 문장의 벡터, 스피커 벡터, 음향 feature에서 뽑은 정보를 담아내며,<br>
  이를 선형 또는 다양한 모양에 대한 예측 모델 또한 존재한다.<br>

- 음성 분석 모듈(Speech Analysis module)<br>
  학습 음성 데이터(Ground-truth)에서 다양한 파라미터를 추출하는데 사용된다.<br>
  Tacotron과 같은 End-to-end 모델에서는 앞뒤 빈 공간을 제거하기도 한다.<br>
  여기서 힘, 강도, 지속시간, 기본 주파수 등을 추출한다.<br><br>

**2) Inference Pipeline and module (생성 단계의 파이프라인)**<br>

- 텍스트 분석 모듈 (Text Analysis module)<br>
  출력 및 음향 모듈을 통해 feature prediction module은 필수적인 음성 표현을 생성한다.<br>
  여기서 출력은 mel-spectrogram, bark scale cepstral, linear scale 크기, 기본 주파수(F0), spectral envelope, 지속시간, 주기성, 음높이 등의 정보를 포함한다.<br>
- 위에서 만들어진 출력은 vocoder의 입력으로 활용된다. 이 모듈에 다양한 버전이 존재하며, end-to-end 시스템에 활용할 때 단일로 사용되는 경우가 많이 있다.<br>
- Deepvoice1과 같은 초기 모델에서는 전통적인 parametric TTS 파이프라인에 의존했지만,<br>
  이후 attention 기반의 seq2seq을 활용하는 Tacotron, Char2wav 등이 등장했다.<br><br>

![model_list_timestamp](/assets/img/speech_survey/model_list_timestamp.png)

위 표는 날짜 별로 발표된 논문에 대한 모델을 정리한 것이다. 참조 또는 응용된 모델은 화살표로 연결되어 있다. <br>
앞에 하얀 네모 박스의 GAN, RNN, Attention, VAE 등 음성합성을 위한 기반이 되는 딥러닝 모델이다.<br><br>

## **5. Introduction to the Mel-Spectrogram**

- Vocoder의 input은 모든 feature의 표현인 mel-spectrogram이다.<br>
  Spectrogram의 오디오 신호에 여러 변환이 이뤄지고, 음성 신호로 생성된다.<br>

- 첫 번째 변환은 STFFT(Short-Term Fast FT)를 사용하여 신호의 스펙트럼을 추출한다.<br>
  이 STFFT는 신호의 다양한 주파수와 그 진폭을 캡처하여 신호를 분해한 것이다.<br>

  ![mel_spectrogram](/assets/img/speech_survey/mel_spectrogram.png)

- 가로축은 시간, 세로축은 주파수 그리고 색상은 데시벨 단위의 신호에 해당된다.<br>

- Mel-spectrogram에서는 frequency scale을 logarithmic scale로 변환한다. <br>
  사람이 인지하기에 500hz → 1000hz는 체감이 크지만, 100500hz → 101000hz는 체감이 없기 때문에 log를 취해 체감에 따른 강도를 비교하기 위해 나타낸다.<br>

- 인간은 주파수의 높낮이의 차이를 잘 체감하지 못하는 점 때문에 mel-scale이 활용되기 시작했다. Mel-scale은 청취자가 같은 거리에서 동일한 피치의 소리를 나타내는 강도를 파악할 수 있기에 자주 활용한다.<br><br>

## **6. Deep Generative Models**

- 음성 생성은 일부 데이터로 아예 관련이 없는 데이터를 만들어내야 하기도 한다.<br>
  22KHz로 10초동안 발음되는 문장은 16bit 기준 440byte의 시퀀스를 생성해야 하는데, 이는 1:14.7의 비율로 생성(?)해야 하는 것을 의미한다.<br>
- 음성 합성은 가장 처음에 CNN을 기반으로 생성되다가, 음성의 연속성을 위해 이전의 문맥을 잇는 개념을 도입하기 위해 RNN이 활용되기 시작했다. 이는 DCNN(Wavenet), VAE(Auto-Encoders), GAN, 픽셀 RNN과 CNN 등 모델로 구현되었다.<br><br>

**1) AR (Auto-Regressive)**<br>

![ar_equation](/assets/img/speech_survey/ar_equation.png)

- 가장 일반적인 구조인 AR(Auto-Regressive)은 다음과 같다.<br>
- Auto-Regressive는 시계열에 대한 regressive이며, 이 시계열은 이전 값들과 관련 있다.<br>
  초기 음성 합성에서 많이 채택했으며, 장기 의존성을 위해 자주 활용했다.<br>
- AR은 구현과 학습이 간단하지만, 잘못된 정보를 그대로 이어갈 수 있고, 최악의 경우 성능이 악화될 수 있다. 또한 직렬 구조로 GPU와 TPU를 활용할 수 없다는 단점이 있다.<br>
  이는 실시간 음성 생성이 불가능하다는 것을 의미한다.<br>
- 이를 극복한 Wavenet과 ClariNet은 이전 세대에 의존하지 않으며, 메모리에 의해서만 제한되면서 병렬화가 가능하다.<br><br>

**2)	DCCN (Dilated Cause Convolution Network)**<br>

![DCNN](/assets/img/speech_survey/DCNN.png)

- Wavenet에서 핵심 되는 기술로, 인접한 input 값이 아니라 일정한 거리를 두어 더 큰 영역에 필터가 적용되는 Dilated Convolution이다. <br>
- 단 몇 개의 레이어로 넓고, 다양하고 넓은 데이터를 활용할 수 있다는 장점이 있어 기술이 발표되고 많은 구조에서 이 모델을 채택했다.<br><br>

**3)	Flow Based Architecture**<br>

![flow_based](/assets/img/speech_survey/flow_based.png)

- Flow 구조는 일련의 가역 변환(invertible transformation)으로 구성된다. Flow의 핵심은 간단한 구조의 가역 변환을 복잡한 가역 변환으로 만드는 것이다.<br>
- NVIDIA는 mel-spectrogram을 음성으로 변환하는 vocoder에서 generative flow를 활용한 waveglow를 발표했다.<br><br>

**4)	Teacher-Student model**<br>

![teacher_student](/assets/img/speech_survey/teacher_student.png)

- 두 가지 모델을 갖는다. 먼저 Non-AR 네트워크인 student 네트워크가 있으며, 다음으로 정확한 정답과 정렬을 학습하는(가이드 역할) pre-trained AR teacher 모델이 존재한다.<br>
- Teacher는 student의 병렬 feed-forward 모델의 결과를 기록하며, student는 학습 기준을 inverse AR flow와 관련 있으며, 다른 flow-based 모델은 waveglow와 함께 발표되었다.<br>
- 그러나 이 병렬 합성 모델의 문제는 invertible 변환에 대한 제한이 존재하는데, 이는 모델의 용량을 제한하는 것이라는 점이다.<br><br>

**5)	Variational Auto-Encoders (VAE)**<br>

![VAE](/assets/img/speech_survey/VAE.png)

- VAE는 auto-encoder를 개조한 것이다. Auto-encoder는 연계하는 2개의 neural network로 구성되어 있다. 이는 평범한 encoder-decoder 모델의 구성 방식이다.<br>
- 음성 학습 단계<br>
  Encoder는 입력을 연속적인 latent reduce(잠재 축소) 표현으로 변환한다. (압축 형식 = Z)<br>
  Decoder는 Z 값에 대한 재구성을 진행하는데, 이 때 최대한 output loss를 줄인다.<br>
  이 때, overlearning을 제한하기 위해 학습 때 mean과 covariance로 정규화 한다.<br>
- 음성 생성 단계<br>
  Encoder는 텍스트를 latent 상태로 변환하고, Decoder는 이 상태를 음향 신호로 변환한다.<br>
- 대부분 이미지 생성과 비슷한 절차를 거치기 때문에 PixelCNN, Glow 등 내용은 TTS의 neural network 아이디어의 근원이 되었다.<br><br>

**6)	Generation Adversarial Network (GAN)**<br>

![GAN](/assets/img/speech_survey/GAN.png)

- GAN은 Generator와 Discriminator를 opposing하는 원리에 근거하는 모델이다.<br>
  Generator는 데이터에서 이미지를 생성하고, Discriminator는 생성된 이미지가 실제에 가까운지를 비교하여 원본인지 가짜인지 판단하도록 학습한다.<br>
- 많은 Vocoder에서 이 기술을 생성 과정에서 활용한다.<br>
- 위 6가지 방법 외에도 가우스 노이즈, IAF와 같은 Markov chain 변환을 통해 신호를 변경하는 확률론적 모델이 있지만 덜 보편적이다.<br>
- 그리고 Attention의 경우, 반복을 할 필요가 없다는 장점으로 seq2seq를 발전시켰지만, 여전히 입력과 출력에 대한 정확한 정렬을 예측하기 어렵다.<br>
- 그래서 그 외 Gaussian Mixture Model(GMM), Hybrid Location-Sensitive Attention, Dynamic Convolution(DCA), Monotonic Attention(MA) 등 알고리즘을 적용한 연구가 진행중이다.<br><br>

![model_list_all](/assets/img/speech_survey/model_list_all.png)

•	16년 9월 DNN 기반의 합성 모델을 시작으로 21년 5월까지의 모델이 나열되어 있다.<br>
•	21년 8월 개최된 Interspeech에 게재된 논문은 포함되지 않았다.<br><br>



다음 글에서 계속